{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy \n",
    "from local_config import *\n",
    "  \n",
    "# assign the values accordingly \n",
    "consumer_key = cons_tok \n",
    "consumer_secret = cons_sec \n",
    "access_token = app_tok \n",
    "access_token_secret = app_sec\n",
    "  \n",
    "# authorization of consumer key and consumer secret \n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret) \n",
    "  \n",
    "# set access to user's access key and access secret  \n",
    "auth.set_access_token(access_token, access_token_secret) \n",
    "  \n",
    "# calling the api  \n",
    "api = tweepy.API(auth) \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read excel file with necessary details\n",
    "excel_file = pd.read_excel('Preliminary Analysis.xlsx', sheet_name='raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataframe for data extracted from excel\n",
    "\n",
    "import datetime\n",
    "import numpy as np\n",
    "g = []\n",
    "for candidate in list(excel_file['Candidate']):\n",
    "    try:\n",
    "        user = api.get_user(candidate.strip().replace(\" \",\"\").upper())\n",
    "        g.append(\n",
    "            {\n",
    "                'Candidate': candidate,\n",
    "                'Followers': user.followers_count,\n",
    "                'Total_tweets': user.statuses_count,\n",
    "                'created_at': user.created_at,\n",
    "                'daily_average_tweets': float(user.statuses_count)/float((datetime.datetime.utcnow() - user.created_at).days)\n",
    "            }\n",
    "        )\n",
    "    except:\n",
    "        g.append(\n",
    "            {\n",
    "                'Candidate': candidate,\n",
    "                'Followers': np.NaN,\n",
    "                'Total_tweets': np.NaN,\n",
    "                'created_at': np.NaN,\n",
    "                'daily_average_tweets':np.NaN\n",
    "            }\n",
    "        )\n",
    "        continue\n",
    "new_df = pd.DataFrame(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.to_excel(r'Followers_Count.xlsx', sheet_name='Candidate_Followers',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get data from twitter\n",
    "\n",
    "for dst in dist_list:\n",
    "    for cmp in comp_list:\n",
    "        try:\n",
    "            user = api.get_user(candidates.at[dst,cmp].strip().replace(\" \",\"\").lower())\n",
    "            followers_count = user.followers_count \n",
    "            print(\"The number of followers of {} are : {}\".format(candidates.at[dst,cmp],str(followers_count)))\n",
    "        except:\n",
    "            print (\"error\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import json\n",
    "\n",
    "client = MongoClient('enter your mongodb url')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n",
      "error\n"
     ]
    }
   ],
   "source": [
    "# save data to mongodb\n",
    "\n",
    "import json\n",
    "\n",
    "for dst in dist_list:\n",
    "    for cmp in comp_list:\n",
    "        try:\n",
    "            user = api.get_user(candidates.at[dst,cmp].strip().replace(\" \",\"\"))\n",
    "            user = user._json\n",
    "            user['Party'] = cmp\n",
    "            db['candidates_'+dst].insert_one(user)\n",
    "        except:\n",
    "            print (\"error\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot bargraph using matplotlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import operator\n",
    "\n",
    "for dst in dist_list:\n",
    "    lst = []\n",
    "    lst = [(name['name']+\" \"+name['affiliation'], name['followers_count']) for name in db['candidates_'+dst].find()]\n",
    "    lst.sort(key = operator.itemgetter(1),reverse = True)\n",
    "    dictform = OrderedDict(lst)\n",
    "    plt.bar(range(len(lst)), list(dictform.values()), align='center')\n",
    "    plt.xticks(range(len(lst)), list(dictform.keys()))\n",
    "    plt.ylabel(\"followers\")\n",
    "    plt.title(dst)\n",
    "    plt.savefig(dst)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to initialize twitter connection \n",
    "\n",
    "def twitter_init():\n",
    "    # assign the values accordingly \n",
    "    consumer_key = cons_tok \n",
    "    consumer_secret = cons_sec \n",
    "    access_token = app_tok \n",
    "    access_token_secret = app_sec\n",
    "      \n",
    "    # authorization of consumer key and consumer secret \n",
    "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret) \n",
    "      \n",
    "    # set access to user's access key and access secret  \n",
    "    auth.set_access_token(access_token, access_token_secret) \n",
    "      \n",
    "    # calling the api  \n",
    "    api = tweepy.API(auth,wait_on_rate_limit=True,wait_on_rate_limit_notify=True)\n",
    "    \n",
    "    return api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for necessary information\n",
    "\n",
    "from datetime import datetime, date, time, timedelta\n",
    "import time \n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tweepy\n",
    "from local_config4 import *\n",
    "\n",
    "api= twitter_init()\n",
    "\n",
    "list1 = ['enter your custom search list']\n",
    "\n",
    "d = []\n",
    "\n",
    "for i in list1:\n",
    "    item = api.get_user(i)\n",
    "    delta = datetime.datetime.utcnow().date() - item.created_at.date()\n",
    "    account_age_days = delta.days\n",
    "    \n",
    "    d.append({\n",
    "        \"name\":item.name,\n",
    "        \"screen_name\": item.screen_name,\n",
    "        \"description\": item.description,\n",
    "        \"statuses_count\": item.statuses_count,\n",
    "        \"friends_count\": item.friends_count,\n",
    "        \"followers_count\": item.followers_count,\n",
    "        \"Average tweets per day\": item.statuses_count/float(account_age_days)\n",
    "    })\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_data = pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracct hashtags and user mentions from returned twitter data\n",
    "\n",
    "hashtags = []\n",
    "mentions = []\n",
    "tweet_count = 0\n",
    "for status in Cursor(api.user_timeline, id=target).items():\n",
    "    tweet_count += 1\n",
    "    if hasattr(status, \"entities\"):\n",
    "        entities = status.entities\n",
    "        if \"hashtags\" in entities:\n",
    "            for ent in entities[\"hashtags\"]:\n",
    "                if ent is not None:\n",
    "                    if \"text\" in ent:\n",
    "                        hashtag = ent[\"text\"]\n",
    "                        if hashtag is not None:\n",
    "                            hashtags.append(hashtag)\n",
    "        if \"user_mentions\" in entities:\n",
    "            for ent in entities[\"user_mentions\"]:\n",
    "                if ent is not None:\n",
    "                    if \"screen_name\" in ent:\n",
    "                        name = ent[\"screen_name\"]\n",
    "                        if name is not None:\n",
    "                            mentions.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NEW',\n",
       " 'DigitalMainStreet',\n",
       " 'BLM',\n",
       " 'covid19',\n",
       " 'VirtualGuelphPride2020',\n",
       " 'PrideMonth',\n",
       " 'safersupply',\n",
       " 'SmallBiz',\n",
       " 'opioidcrisis',\n",
       " 'AccessAbility',\n",
       " 'COVID19',\n",
       " 'COVID19Guelph',\n",
       " 'HarmReduction',\n",
       " 'guelph',\n",
       " 'ICYMI',\n",
       " 'LTC',\n",
       " 'FirstNations',\n",
       " 'WorldEnvironmentDay',\n",
       " 'antiracism',\n",
       " 'migrantworkers',\n",
       " 'UprootElderAbuse',\n",
       " 'MentalHealth',\n",
       " 'FixLTC',\n",
       " 'Inuit',\n",
       " 'InfuseTheSystemwithLoveandCare',\n",
       " 'pandemic',\n",
       " 'BlackExperienceProject',\n",
       " 'smallbusinesseveryday',\n",
       " 'onpoli',\n",
       " 'smallbiz',\n",
       " 'MÃ©tis',\n",
       " 'Covid_19',\n",
       " 'ParamedicServicesWeek',\n",
       " 'SolidarityWithBIPOC',\n",
       " 'LongTermCare',\n",
       " 'harmreduction',\n",
       " 'FirstNation',\n",
       " 'SmallBusinessEveryDay',\n",
       " 'BlackOutTuesday',\n",
       " 'publichealth',\n",
       " 'Guelph',\n",
       " 'nonprofit']"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashtags = list(set(hashtags))\n",
    "hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['annie9190',\n",
       " 'vote_victoria',\n",
       " 'Raptors',\n",
       " 'TORHousing',\n",
       " 'AustonWhite3',\n",
       " 'AEFO_ON_CA',\n",
       " 'OntarioGreens',\n",
       " 'guelphtransit',\n",
       " 'CityPlanTO',\n",
       " 'ORCRoseAnne',\n",
       " 'ChildCareON',\n",
       " 'AdriannaTetley',\n",
       " 'kimbalmack',\n",
       " 'jf_hoops',\n",
       " 'stefcyclops',\n",
       " 'HousingNowTO',\n",
       " 'RNAO',\n",
       " 'CarolynWea',\n",
       " 'martinkouprie',\n",
       " 'tylerwhat16',\n",
       " 'activeguelph',\n",
       " 'OntHumanRights',\n",
       " 'leannepiper',\n",
       " 'SteveClarkPC',\n",
       " 'GuelphBHS',\n",
       " 'familycounsGW',\n",
       " '1Granny_Connie',\n",
       " 'fightbacklibby',\n",
       " 'modernfarmer',\n",
       " '_Antny__',\n",
       " 'firebladeholste',\n",
       " 'CarpenterRePete',\n",
       " 'PhilAlltWard3',\n",
       " 'RichRaycraft',\n",
       " 'Chonilla',\n",
       " 'BarberRino',\n",
       " 'TheAgenda',\n",
       " 'hello_sane',\n",
       " 'GuelphGeography',\n",
       " 'johngaltps',\n",
       " 'TheElders',\n",
       " 'ONPsych',\n",
       " 'MikeSchreiner',\n",
       " 'PattyHajdu',\n",
       " 'cathyacrowe',\n",
       " 'WDGPublicHealth',\n",
       " 'AnnamiePaul',\n",
       " 'R_Mallough',\n",
       " 'JoAnnRobertsHFX',\n",
       " 'King_Reynolds23',\n",
       " 'OECTAProv',\n",
       " 'MABurrowes',\n",
       " 'progresstoronto',\n",
       " 'brenda_slomka',\n",
       " 'GuelphCHC',\n",
       " 'philosogilvie',\n",
       " 'ElizabethMay',\n",
       " 'GNighthawks',\n",
       " 'JustinTrudeau',\n",
       " 'RaechelleDev',\n",
       " 'bow_canadian',\n",
       " 'cathmckenna',\n",
       " 'LaurieScottPC',\n",
       " 'Sreerag157',\n",
       " 'CameronKusch',\n",
       " 'AbhijeetMonet',\n",
       " 'MitzieHunter',\n",
       " 'cancivlib',\n",
       " 'cityofguelph',\n",
       " 'HonAhmedHussen',\n",
       " 'PaulTaylorTO',\n",
       " 'NelsonPK20',\n",
       " 'robertbenzie',\n",
       " 'CyndyForsyth1',\n",
       " 'AManCalledH',\n",
       " 'ArtNotShame',\n",
       " 'ARCHGuelph',\n",
       " 'TGreen3_',\n",
       " 'TAEHomelessness',\n",
       " 'TdotCanuck',\n",
       " 'Bill_Morneau',\n",
       " 'l_stone',\n",
       " 'kwame_mckenzie',\n",
       " 'stcrawford2',\n",
       " 'aodaalliance',\n",
       " 'Travisdhanraj',\n",
       " 'o_n_n',\n",
       " 'UARRToronto',\n",
       " 'wwickbett',\n",
       " 'bannon98',\n",
       " 'F4KOntario',\n",
       " 'mattnicholas',\n",
       " 'coteau',\n",
       " 'namshine',\n",
       " 'MorganPCampbell',\n",
       " 'AllianceON',\n",
       " 'gpsmedia',\n",
       " 'LAENToronto',\n",
       " 'BarbaraDMathews',\n",
       " 'derekpokora',\n",
       " 'geobandito',\n",
       " 'GuelphChamber',\n",
       " 'DanaMcCauley',\n",
       " 'OI_Mike13',\n",
       " 'JR_Ottawa',\n",
       " 'jrasevych',\n",
       " 'DavidSuzukiFDN',\n",
       " 'velmamorgan',\n",
       " 'FdnofGGH',\n",
       " 'celliottability',\n",
       " 'SCMirror',\n",
       " 'ManteMolepo',\n",
       " 'KirkconnellRoss',\n",
       " 'erin',\n",
       " 'spaikin',\n",
       " 'Glen4Climate',\n",
       " 'kahnkidz',\n",
       " 'MPPArnottWHH',\n",
       " 'gwwic',\n",
       " 'GuelphGeneral',\n",
       " 'coachkissi',\n",
       " '_CreateTO',\n",
       " 'RotaryGTrillium',\n",
       " 'ChildrensFdnGW',\n",
       " 'misstyszka',\n",
       " 'CamGuthrie',\n",
       " 'RachelleManios',\n",
       " 'DavidPiccini',\n",
       " 'MichaleKyser',\n",
       " 'solmamakwa',\n",
       " 'wellingtncounty',\n",
       " 'libbyznaimer',\n",
       " 'LakesideHH',\n",
       " 'cfibON',\n",
       " 'DowntownGuelph',\n",
       " 'TOCentreGreens',\n",
       " 'DavidLepofsky',\n",
       " 'LloydLongfield',\n",
       " 'WGDrugStrategy',\n",
       " 'MarvaWisdom',\n",
       " 'ChiefsofOntario',\n",
       " 'ETFOeducators',\n",
       " 'globalecochange',\n",
       " 'CanadianGreens',\n",
       " 'thatjamesgordon',\n",
       " 'CrossBorderPod',\n",
       " 'rovingprofessor',\n",
       " 'ABPA_NONT',\n",
       " 'richardzussman',\n",
       " 'LyndseyS2020',\n",
       " 'alectranews',\n",
       " 'osstf',\n",
       " 'darcyphiggins',\n",
       " 'joshnamaharaj',\n",
       " 'Sflecce',\n",
       " 'MikeDarmon1',\n",
       " 'Seanmitc',\n",
       " 'fordnation',\n",
       " 'jen_keesmaat']"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mentions = list(set(mentions))\n",
    "mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping using search criteria\n",
    "\n",
    "def scraptweets(search_words, date_since, numTweets, numRuns):\n",
    "    \n",
    "    # Define a for-loop to generate tweets at regular intervals\n",
    "    # We cannot make large API call in one go. Hence, let's try T times\n",
    "    \n",
    "    # Define a pandas dataframe to store the date:\n",
    "    db_tweets = pd.DataFrame(columns = ['username', 'acctdesc', 'location', 'following',\n",
    "                                        'followers', 'totaltweets', 'usercreatedts', 'tweetcreatedts',\n",
    "                                        'retweetcount', 'text', 'hashtags']\n",
    "                                )\n",
    "    program_start = time.time()\n",
    "    for i in range(0, numRuns):\n",
    "        # We will time how long it takes to scrape tweets for each run:\n",
    "        start_run = time.time()\n",
    "        \n",
    "        # Collect tweets using the Cursor object\n",
    "        # .Cursor() returns an object that you can iterate or loop over to access the data collected.\n",
    "        # Each item in the iterator has various attributes that you can access to get information about each tweet\n",
    "        tweets = tweepy.Cursor(api.search, q=search_words, lang=\"en\", since=date_since, tweet_mode='extended').items(numTweets)\n",
    "# Store these tweets into a python list\n",
    "        tweet_list = [tweet for tweet in tweets]\n",
    "# Obtain the following info (methods to call them out):\n",
    "        # user.screen_name - twitter handle\n",
    "        # user.description - description of account\n",
    "        # user.location - where is he tweeting from\n",
    "        # user.friends_count - no. of other users that user is following (following)\n",
    "        # user.followers_count - no. of other users who are following this user (followers)\n",
    "        # user.statuses_count - total tweets by user\n",
    "        # user.created_at - when the user account was created\n",
    "        # created_at - when the tweet was created\n",
    "        # retweet_count - no. of retweets\n",
    "        # (deprecated) user.favourites_count - probably total no. of tweets that is favourited by user\n",
    "        # retweeted_status.full_text - full text of the tweet\n",
    "        # tweet.entities['hashtags'] - hashtags in the tweet\n",
    "# Begin scraping the tweets individually:\n",
    "        noTweets = 0\n",
    "for tweet in tweet_list:\n",
    "# Pull the values\n",
    "            username = tweet.user.screen_name\n",
    "            acctdesc = tweet.user.description\n",
    "            location = tweet.user.location\n",
    "            following = tweet.user.friends_count\n",
    "            followers = tweet.user.followers_count\n",
    "            totaltweets = tweet.user.statuses_count\n",
    "            usercreatedts = tweet.user.created_at\n",
    "            tweetcreatedts = tweet.created_at\n",
    "            retweetcount = tweet.retweet_count\n",
    "            hashtags = tweet.entities['hashtags']\n",
    "try:\n",
    "                text = tweet.retweeted_status.full_text\n",
    "            except AttributeError:  # Not a Retweet\n",
    "                text = tweet.full_text\n",
    "# Add the 11 variables to the empty list - ith_tweet:\n",
    "            ith_tweet = [username, acctdesc, location, following, followers, totaltweets,\n",
    "                         usercreatedts, tweetcreatedts, retweetcount, text, hashtags]\n",
    "# Append to dataframe - db_tweets\n",
    "            db_tweets.loc[len(db_tweets)] = ith_tweet\n",
    "# increase counter - noTweets  \n",
    "            noTweets += 1\n",
    "        \n",
    "        # Run ended:\n",
    "        end_run = time.time()\n",
    "        duration_run = round((end_run-start_run)/60, 2)\n",
    "        \n",
    "        print('no. of tweets scraped for run {} is {}'.format(i + 1, noTweets))\n",
    "        print('time take for {} run to complete is {} mins'.format(i+1, duration_run))\n",
    "        \n",
    "        time.sleep(920) #15 minute sleep time\n",
    "# Once all runs have completed, save them to a single csv file:\n",
    "    from datetime import datetime\n",
    "    \n",
    "    # Obtain timestamp in a readable format\n",
    "    to_csv_timestamp = datetime.today().strftime('%Y%m%d_%H%M%S')\n",
    "# Define working path and filename\n",
    "    path = os.getcwd()\n",
    "    filename = path + '/data/' + to_csv_timestamp + '_sahkprotests_tweets.csv'\n",
    "# Store dataframe in csv with creation date timestamp\n",
    "    db_tweets.to_csv(filename, index = False)\n",
    "    \n",
    "    program_end = time.time()\n",
    "    print('Scraping has completed!')\n",
    "    print('Total time taken to scrap is {} minutes.'.format(round(program_end - program_start)/60, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stream listener for scraping\n",
    "\n",
    "from __future__ import print_function\n",
    "import tweepy\n",
    "import json\n",
    "from pymongo import MongoClient\n",
    "\n",
    "MONGO_HOST= 'mongodb://localhost/twitterdb'  # assuming you have mongoDB installed locally\n",
    "                                             # and a database called 'twitterdb'\n",
    "\n",
    "WORDS = ['#bigdata', '#AI', '#datascience', '#machinelearning', '#ml', '#iot']\n",
    "\n",
    "CONSUMER_KEY = \"KEY\"\n",
    "CONSUMER_SECRET = \"SECRET\"\n",
    "ACCESS_TOKEN = \"TOKEN\"\n",
    "ACCESS_TOKEN_SECRET = \"TOKEN_SECRET\"\n",
    "\n",
    "\n",
    "class StreamListener(tweepy.StreamListener):    \n",
    "    #This is a class provided by tweepy to access the Twitter Streaming API. \n",
    "\n",
    "    def on_connect(self):\n",
    "        # Called initially to connect to the Streaming API\n",
    "        print(\"You are now connected to the streaming API.\")\n",
    " \n",
    "    def on_error(self, status_code):\n",
    "        # On error - if an error occurs, display the error / status code\n",
    "        print('An Error has occured: ' + repr(status_code))\n",
    "        return False\n",
    " \n",
    "    def on_data(self, data):\n",
    "        #This is the meat of the script...it connects to your mongoDB and stores the tweet\n",
    "        try:\n",
    "            client = MongoClient(MONGO_HOST)\n",
    "            \n",
    "            # Use twitterdb database. If it doesn't exist, it will be created.\n",
    "            db = client.twitterdb\n",
    "    \n",
    "            # Decode the JSON from Twitter\n",
    "            datajson = json.loads(data)\n",
    "            \n",
    "            #grab the 'created_at' data from the Tweet to use for display\n",
    "            created_at = datajson['created_at']\n",
    "\n",
    "            #print out a message to the screen that we have collected a tweet\n",
    "            print(\"Tweet collected at \" + str(created_at))\n",
    "            \n",
    "            #insert the data into the mongoDB into a collection called twitter_search\n",
    "            #if twitter_search doesn't exist, it will be created.\n",
    "            db.twitter_search.insert(datajson)\n",
    "        except Exception as e:\n",
    "           print(e)\n",
    "\n",
    "auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "auth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n",
    "#Set up the listener. The 'wait_on_rate_limit=True' is needed to help with Twitter API rate limiting.\n",
    "listener = StreamListener(api=tweepy.API(wait_on_rate_limit=True)) \n",
    "streamer = tweepy.Stream(auth=auth, listener=listener)\n",
    "print(\"Tracking: \" + str(WORDS))\n",
    "streamer.filter(track=WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search stuff\n",
    "searched_tweets = [status._json for status in tweepy.Cursor(api.search,  q=\"Python\").items(5)]\n",
    "json_strings = [json.dumps(json_obj) for json_obj in searched_tweets] "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
